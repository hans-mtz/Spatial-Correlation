## Power of the test using SCPC

@Muller2022JBES discuss briefly the power of the tests using their variance estimator. They discuss the difference in the power of the tests using the conditional and the unconditional SCPC, the trade-off in the length of the CI and the number of the principal components used to calculate $\sigma_{SCPC}$, and the expected length of the CI as a function of $\bar\rho_{max}$.

### Conditional vs. Unconditional SCPC (p.8)

The authors argue that the size-adjusted power (average length of the confidence intervals) of SCPC and C-SCPC (conditional) are identical because both methods are based on the same *t*-statistic and only differ in their critical values. The authors argue that the differences in the average confidence interval lengths of both methods are small. They based their argument on an unreported comparison realized for only one experiment design (p. 8). 

The authors acknowledge that the size-adjusted Kernel method is "somewhat" more efficient than SCPC and C-SCPC. The authors argue however that this is not a reason to prefer the Kernel method because in practice the "size-adjustment that adjust for the larger bias in the $\hat\sigma_{K}^2$" is not feasible. 

### Trade-off in choosing q (p.4)

The authors discuss that there is a trade-off between the number of principal components $q$ and the expected length of the 95% confidence intervals. In particular, for a fixed critical value, the expected length of the confidence interval falls as $q$ increases, but larger $q$ requires larger critical values to control coverage. The authors suggest that minimizing the expected 95% confidence interval length under the iid benchmark yields a value of $q$ that works well for a range of values of $c$.

### Expected length of confidence intervals as a function of $\bar\rho_{max}$ (p.11,12)

The authors also discuss what happens to the rejection frequencies and confidence intervals if the researcher misjudges the spatial correlation $\bar\rho_{max}$. For example, if the researcher provides a significantly low average pairwise correlation $\bar\rho_{max}=0.03$ when the true is actually higher $\bar\rho=0.10$, the authors argue that the average rejection increases but marginally. The authors performed a set of 14 experiments with different DGPs, first with the true $\bar\rho=0.03$ and then increased to 0.10, but they kept the provided $\bar\rho_{max}=0.03$. They observed that the average rejection frequency increased from 0.04 to 0.07. However, when you look at the supplemental materials the rejection frequency for some of the experiments went up even above 0.4.

The authors also discuss how the confidence intervals increase if the researcher gives a higher value of $\bar\rho_{max}$ than the true $\bar\rho$. The authors compare the CI generated by the C-SCPC method to the length of the oracle $\pm 1.96$ interval length when the $\bar\rho_{max}={0.1, 0.03,0.01,0.003}$. The authors find that the average C-SCPC CI is 1.55, 1.2, 1.1, and 1.05 times larger than the oracle $\pm 1.96$ CI. The authors conclude that the cost of the default value of $\bar\rho=0.03$ is about a 20% increase in the CI lengths.


